#!/bin/bash
# Complete Batch Layer Pipeline Test
# Tests: Bronze ‚Üí Silver ‚Üí Gold ‚Üí MongoDB data flow

set -e
cd /home/dmv/Documents/GitHub/movie-data-analysis-pipeline/layers/batch_layer

echo "========================================="
echo "üöÄ BATCH LAYER COMPLETE PIPELINE TEST"
echo "========================================="
echo ""
echo "Services:"
echo "- Airflow UI: http://localhost:8088 (admin/admin)"
echo "- MinIO UI: http://localhost:9001 (minioadmin/minioadmin)"
echo "- MongoDB: mongodb://admin:password@localhost:27017/moviedb"
echo ""

# Function to check service health
check_services() {
    echo "üìã Checking service health..."
    sudo docker compose -f docker-compose.batch.yml ps --format "table {{.Name}}\t{{.Status}}"
    echo ""
}

# Function to check MinIO buckets
check_minio_buckets() {
    echo "üì¶ Checking MinIO buckets..."
    sudo docker logs minio-init 2>&1 | tail -5
    echo ""
}

# Run Bronze ingestion
run_bronze() {
    echo "========================================="
    echo "1Ô∏è‚É£  BRONZE LAYER INGESTION"
    echo "========================================="
    echo "Fetching data from TMDB API..."
    
    sudo docker exec pyspark-runner /bin/bash -c "
        cd /opt/spark-jobs && \
        export PYTHONPATH=/opt/spark-jobs && \
        python3 bronze_ingest.py --pages 2
    " 2>&1 | grep -E "(INFO|ERROR|movies_fetched|Bronze)" | tail -20
    
    echo ""
    echo "‚úÖ Bronze ingestion completed!"
    echo ""
}

# Run Silver transformation
run_silver() {
    echo "========================================="
    echo "2Ô∏è‚É£  SILVER LAYER TRANSFORMATION"
    echo "========================================="
    echo "Cleaning and enriching data..."
    
    sudo docker exec pyspark-runner /bin/bash -c "
        cd /opt/spark-jobs && \
        export PYTHONPATH=/opt/spark-jobs && \
        python3 silver_transform.py
    " 2>&1 | grep -E "(INFO|ERROR|Silver)" | tail -20
    
    echo ""
    echo "‚úÖ Silver transformation completed!"
    echo ""
}

# Run Gold aggregation
run_gold() {
    echo "========================================="
    echo "3Ô∏è‚É£  GOLD LAYER AGGREGATION"
    echo "========================================="
    echo "Computing analytics and aggregations..."
    
    sudo docker exec pyspark-runner /bin/bash -c "
        cd /opt/spark-jobs && \
        export PYTHONPATH=/opt/spark-jobs && \
        python3 gold_aggregate.py
    " 2>&1 | grep -E "(INFO|ERROR|Gold)" | tail -20
    
    echo ""
    echo "‚úÖ Gold aggregation completed!"
    echo ""
}

# Run MongoDB export
run_mongo_export() {
    echo "========================================="
    echo "4Ô∏è‚É£  MONGODB EXPORT"
    echo "========================================="
    echo "Exporting data to serving layer..."
    
    sudo docker exec pyspark-runner /bin/bash -c "
        cd /opt/spark-jobs && \
        export PYTHONPATH=/opt/spark-jobs && \
        python3 export_to_mongo.py
    " 2>&1 | grep -E "(INFO|ERROR|MongoDB|Exported)" | tail -20
    
    echo ""
    echo "‚úÖ MongoDB export completed!"
    echo ""
}

# Verify data at each layer
verify_data() {
    echo "========================================="
    echo "5Ô∏è‚É£  DATA VERIFICATION"
    echo "========================================="
    echo ""
    
    echo "üìä Bronze Layer (MinIO s3a://bronze-data/):"
    sudo docker exec pyspark-runner python3 -c "
import sys
sys.path.insert(0, '/opt/spark-jobs')
from utils.spark_session import get_spark_session

spark = get_spark_session('verify_bronze')
try:
    movies = spark.read.parquet('s3a://bronze-data/movies/')
    print(f'  ‚úÖ Movies: {movies.count()} records')
    
    details = spark.read.parquet('s3a://bronze-data/movie_details/')
    print(f'  ‚úÖ Details: {details.count()} records')
    
    credits = spark.read.parquet('s3a://bronze-data/credits/')
    print(f'  ‚úÖ Credits: {credits.count()} records')
    
    reviews = spark.read.parquet('s3a://bronze-data/reviews/')
    print(f'  ‚úÖ Reviews: {reviews.count()} records')
except Exception as e:
    print(f'  ‚ùå Error: {str(e)[:100]}')
finally:
    spark.stop()
" 2>&1 | grep -E "(‚úÖ|‚ùå|Movies|Details|Credits|Reviews)"
    echo ""
    
    echo "üìä Silver Layer (MinIO s3a://silver-data/):"
    sudo docker exec pyspark-runner python3 -c "
import sys
sys.path.insert(0, '/opt/spark-jobs')
from utils.spark_session import get_spark_session

spark = get_spark_session('verify_silver')
try:
    movies = spark.read.parquet('s3a://silver-data/movies/')
    print(f'  ‚úÖ Movies: {movies.count()} records')
    
    reviews = spark.read.parquet('s3a://silver-data/reviews/')
    print(f'  ‚úÖ Reviews: {reviews.count()} records')
except Exception as e:
    print(f'  ‚ùå Error: {str(e)[:100]}')
finally:
    spark.stop()
" 2>&1 | grep -E "(‚úÖ|‚ùå|Movies|Reviews)"
    echo ""
    
    echo "üìä Gold Layer (MinIO s3a://gold-data/):"
    sudo docker exec pyspark-runner python3 -c "
import sys
sys.path.insert(0, '/opt/spark-jobs')
from utils.spark_session import get_spark_session

spark = get_spark_session('verify_gold')
try:
    gold = spark.read.parquet('s3a://gold-data/')
    count = gold.count()
    print(f'  ‚úÖ Aggregations: {count} records')
    
    # Show metrics breakdown
    metrics = gold.groupBy('view_type').count().collect()
    for row in metrics:
        print(f'     - {row[\"view_type\"]}: {row[\"count\"]} records')
except Exception as e:
    print(f'  ‚ùå Error: {str(e)[:100]}')
finally:
    spark.stop()
" 2>&1 | grep -E "(‚úÖ|‚ùå|Aggregations|-)"
    echo ""
    
    echo "üìä MongoDB (moviedb.batch_views):"
    sudo docker exec mongodb mongosh --quiet --eval "
        db = db.getSiblingDB('moviedb');
        db.auth('admin', 'password');
        const count = db.batch_views.countDocuments();
        print('  ‚úÖ Documents: ' + count);
        
        // Count by view_type
        const types = db.batch_views.distinct('view_type');
        types.forEach(function(type) {
            const typeCount = db.batch_views.countDocuments({view_type: type});
            print('     - ' + type + ': ' + typeCount + ' documents');
        });
    " 2>&1 | grep -E "(‚úÖ|‚ùå|-)"
    echo ""
}

# Show sample data
show_samples() {
    echo "========================================="
    echo "6Ô∏è‚É£  SAMPLE DATA"
    echo "========================================="
    echo ""
    
    echo "üìÑ Bronze Sample (first 3 movies):"
    sudo docker exec pyspark-runner python3 -c "
import sys
sys.path.insert(0, '/opt/spark-jobs')
from utils.spark_session import get_spark_session

spark = get_spark_session('sample')
movies = spark.read.parquet('s3a://bronze-data/movies/')
movies.select('id', 'title', 'release_date', 'vote_average').show(3, truncate=False)
spark.stop()
" 2>&1 | tail -10
    echo ""
    
    echo "üìÑ MongoDB Sample (first document):"
    sudo docker exec mongodb mongosh --quiet --eval "
        db = db.getSiblingDB('moviedb');
        db.auth('admin', 'password');
        printjson(db.batch_views.findOne({}, {_id: 0}));
    " 2>&1 | head -20
    echo ""
}

# Main execution
main() {
    check_services
    check_minio_buckets
    
    echo "Starting pipeline execution..."
    echo ""
    
    run_bronze
    run_silver
    run_gold
    run_mongo_export
    
    verify_data
    show_samples
    
    echo "========================================="
    echo "‚úÖ PIPELINE TEST COMPLETED SUCCESSFULLY!"
    echo "========================================="
    echo ""
    echo "üìä Access your data:"
    echo "   ‚Ä¢ Airflow UI: http://localhost:8088"
    echo "   ‚Ä¢ MinIO Console: http://localhost:9001"
    echo "   ‚Ä¢ MongoDB: mongosh mongodb://admin:password@localhost:27017/moviedb?authSource=admin"
    echo ""
    echo "üêõ Report any bugs you find!"
}

# Run main
main
