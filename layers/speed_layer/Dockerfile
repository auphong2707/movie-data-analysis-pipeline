# =====================================================================
# Speed Layer Dockerfile - Spark Structured Streaming Pipeline
# =====================================================================
# M√¥ t·∫£: Container cho Speed Layer x·ª≠ l√Ω streaming data t·ª´ Kafka
# Stack: Python 3.10 + Java 11 + PySpark 3.4.4 + Kafka + Cassandra 4.x
# =====================================================================

# ---------------------------------------------------------------------
# B∆Ø·ªöC 1: Base Image - Python 3.10 tr√™n Debian Bullseye
# ---------------------------------------------------------------------
# L√Ω do ch·ªçn bullseye: H·ªó tr·ª£ OpenJDK 11 ·ªïn ƒë·ªãnh t·ª´ repository ch√≠nh th·ª©c
FROM python:3.10-slim-bullseye

# ---------------------------------------------------------------------
# B∆Ø·ªöC 2: C√†i ƒë·∫∑t OpenJDK 11 v√† c√°c c√¥ng c·ª• c·∫ßn thi·∫øt
# ---------------------------------------------------------------------
# Java 11: Phi√™n b·∫£n ·ªïn ƒë·ªãnh nh·∫•t cho Spark 3.4.x (tr√°nh l·ªói class version)
# procps: Cung c·∫•p ps, top ƒë·ªÉ monitoring containers
# curl: Download dependencies ho·∫∑c health checks
RUN apt-get update && apt-get install -y \
    openjdk-11-jdk \
    procps \
    curl \
    && rm -rf /var/lib/apt/lists/*

# ---------------------------------------------------------------------
# B∆Ø·ªöC 3: Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng Java
# ---------------------------------------------------------------------
# JAVA_HOME: ƒê·∫£m b·∫£o Spark t√¨m ƒë√∫ng Java runtime
# PATH: Cho ph√©p g·ªçi java, javac t·ª´ command line
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV PATH=$PATH:$JAVA_HOME/bin

# ---------------------------------------------------------------------
# B∆Ø·ªöC 4: T·∫°o th∆∞ m·ª•c l√†m vi·ªác
# ---------------------------------------------------------------------
WORKDIR /app

# ---------------------------------------------------------------------
# B∆Ø·ªöC 5: C√†i ƒë·∫∑t Python dependencies
# ---------------------------------------------------------------------
# PySpark 3.4.4: Phi√™n b·∫£n stable, t∆∞∆°ng th√≠ch Java 11 v√† Cassandra 4.x
# kafka-python: Consumer/Producer Kafka messages  
# confluent-kafka: Kafka producers v·ªõi Avro serialization (for tmdb producer)
# fastavro: Required by confluent-kafka for Avro serialization
# cassandra-driver: Connector cho Apache Cassandra
# --no-cache-dir: Gi·∫£m image size
# --prefer-binary: ∆Øu ti√™n wheel packages, build nhanh h∆°n
RUN pip install --no-cache-dir --prefer-binary \
    pyspark==3.4.4 \
    kafka-python==2.0.2 \
    confluent-kafka==2.3.0 \
    fastavro==1.9.4 \
    cassandra-driver==3.29.1 \
    vaderSentiment==3.3.2 \
    pyyaml==6.0.1 \
    requests==2.31.0 \
    avro==1.11.3 \
    python-dotenv==1.0.0

# ---------------------------------------------------------------------
# B∆Ø·ªöC 6: Copy source code v√†o container
# ---------------------------------------------------------------------
# Copy th∆∞ m·ª•c streaming_jobs (ch·ª©a logic x·ª≠ l√Ω streaming)
COPY streaming_jobs/ /app/streaming_jobs/

# Copy c√°c modules h·ªó tr·ª£
COPY kafka_producers/ /app/kafka_producers/
COPY cassandra_views/ /app/cassandra_views/

# Copy requirements.txt ƒë·ªÉ reference (optional)
COPY requirements.txt /app/

# Note: config/ folder will be mounted as volume in docker-compose.yml
# so we don't copy it here to avoid conflicts

# ---------------------------------------------------------------------
# B∆Ø·ªöC 7: T·∫°o th∆∞ m·ª•c logs v√† checkpoints
# ---------------------------------------------------------------------
# logs/: L∆∞u application logs
# checkpoints/: Spark Structured Streaming checkpoints (fault-tolerance)
RUN mkdir -p /app/logs /app/checkpoints

# ---------------------------------------------------------------------
# B∆Ø·ªöC 8: Thi·∫øt l·∫≠p PYTHONPATH
# ---------------------------------------------------------------------
# ƒê·∫£m b·∫£o Python t√¨m th·∫•y modules t·ª´ /app v√† /app/config
ENV PYTHONPATH=/app:/app/config

# ---------------------------------------------------------------------
# B∆Ø·ªöC 9: Expose port Spark UI
# ---------------------------------------------------------------------
# Port 4040: Spark Web UI ƒë·ªÉ monitoring streaming jobs
EXPOSE 4040

# ---------------------------------------------------------------------
# B∆Ø·ªöC 10: Health Check
# ---------------------------------------------------------------------
# Ki·ªÉm tra container c√≤n s·ªëng kh√¥ng (m·ªói 30s)
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
  CMD python -c "import sys; sys.exit(0)" || exit 1

# ---------------------------------------------------------------------
# B∆Ø·ªöC 11: Default Command
# ---------------------------------------------------------------------
# Default: show help/version (docker-compose will override with specific job)
# Docker Compose services override this with specific streaming job
CMD ["python3", "--version"]

# =====================================================================
# üöÄ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:
# =====================================================================
# Build image:
#   docker build -t speed-layer:latest .
#
# Run standalone:
#   docker run -it --network=host speed-layer:latest
#
# D√πng v·ªõi Docker Compose:
#   Trong docker-compose.yml, override CMD n·∫øu c·∫ßn:
#   command: ["python3", "streaming_jobs/movie_aggregation_stream.py"]
#
# D√πng spark-submit v·ªõi Cassandra connector:
#   docker run -it speed-layer:latest spark-submit \
#     --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 \
#     --conf spark.cassandra.connection.host=cassandra \
#     streaming_jobs/movie_aggregation_stream.py
# =====================================================================
