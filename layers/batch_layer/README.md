# Batch Layer - Movie Data Analysis Pipeline# Batch Layer - Historical Data Processing



Complete batch processing layer for ingesting, transforming, and serving movie analytics data from TMDB API using the Lambda Architecture pattern.## Table of Contents



---- [Overview](#overview)

- [Quick Start with Docker](#quick-start-with-docker)

## ğŸ¯ Overview- [Architecture](#architecture)

- [Data Layers](#data-layers)

The Batch Layer implements a multi-stage data pipeline that:- [Manual Usage](#manual-usage)

1. **Ingests** movie data from TMDB API (Bronze Layer)- [Development](#development)

2. **Transforms** and cleans data with quality checks (Silver Layer)  

3. **Aggregates** analytics by genre, trending, and temporal metrics (Gold Layer)## Overview

4. **Serves** pre-computed views via MongoDB for fast queries

The **Batch Layer** is responsible for processing the complete historical dataset to generate accurate, comprehensive views. It runs periodically (every 4 hours) and prioritizes accuracy over latency.

### Architecture

---

```

TMDB API â†’ Bronze (Raw) â†’ Silver (Clean) â†’ Gold (Aggregated) â†’ MongoDB (Serving)## Quick Start with Docker

            HDFS           HDFS             HDFS                  NoSQL DB

```### Prerequisites



---- Docker 24.0+ and Docker Compose 2.0+

- At least 8GB RAM allocated to Docker

## ğŸš€ Quick Start- 20GB free disk space

- TMDB API Key (already configured: `36bdc639ae379da0a89bfb9c556e2136`)

### Prerequisites

### Start the Stack

- Docker & Docker Compose 2.0+

- 8GB+ RAM recommended```bash

- 20GB+ free disk space# Navigate to batch layer directory

- TMDB API key (get from https://www.themoviedb.org/settings/api)cd layers/batch_layer



### 1. Setup Environment# Start all services (HDFS, Spark, Airflow, MongoDB)

make up

```bash

# Navigate to batch layer directory# Check service status

cd layers/batch_layermake ps



# Copy environment template# View service URLs

cp .env.example .envmake ui

```

# Edit .env and add your TMDB API key

nano .env  # or vim, code, etc.### Service URLs

```

Once all services are running:

**Required:** Update `TMDB_API_KEY` in `.env` file.

| Service | URL | Credentials |

### 2. Start Services|---------|-----|-------------|

| **Airflow UI** | http://localhost:8080 | admin / admin |

```bash| **Spark Master UI** | http://localhost:8081 | - |

# Start all services (HDFS, Spark, Airflow, MongoDB)| **HDFS Namenode UI** | http://localhost:9870 | - |

sudo docker compose -f docker-compose.batch.yml up -d| **Mongo Express** | http://localhost:8082 | admin / admin |

| **MongoDB** | mongodb://localhost:27017 | admin / password |

# Wait for services to be healthy (~30-60 seconds)

sudo docker compose -f docker-compose.batch.yml ps### Run Your First Pipeline

```

```bash

### 3. Verify Deployment# Wait for all services to be healthy (2-3 minutes)

make health

```bash

# Check all services are running# Create HDFS directories

sudo docker compose -f docker-compose.batch.yml psmake hdfs-create-dirs



# Expected: 11 services with status "Up" or "Healthy"# Trigger the batch pipeline DAG

```make trigger-dag



All services should show status "Up (healthy)":# Monitor logs

- âœ… HDFS: namenode + 3 datanodes  make logs-airflow

- âœ… Spark: master + 2 workers

- âœ… Airflow: webserver + scheduler + init# Check HDFS data

- âœ… PostgreSQL (Airflow metadata)make hdfs-ls

- âœ… MongoDB + Mongo Express

# Query MongoDB batch_views

---make mongo-query-views

```

## ğŸ“Š Data Pipeline

### Common Commands

### Input Source

```bash

**TMDB API** - The Movie Database API# View all available commands

- **Endpoints**: `/movie/popular`, `/movie/top_rated`, `/movie/now_playing`make help

- **Rate Limit**: 4 requests/second

- **Volume**: ~1,200 movies per 4-hour batch run (20 pages Ã— 3 categories)# Follow logs for specific services

make logs-airflow    # Airflow logs

**Sample API Response Fields:**make logs-spark      # Spark logs

```jsonmake logs-hdfs       # HDFS logs

{make logs-mongo      # MongoDB logs

  "id": 550,

  "title": "Fight Club",# Run a backfill for a date range

  "release_date": "1999-10-15",make backfill START=2024-01-01 END=2024-01-31

  "vote_average": 8.4,

  "vote_count": 26280,# Stop all services

  "popularity": 61.416,make down

  "genre_ids": [18, 53, 35],

  "overview": "A ticking-time-bomb insomniac...",# Stop and remove all data

  ...make down-v

}

```# Rebuild images

make build

### Processing Layers

# Open shells

#### ğŸ¥‰ Bronze Layer (Raw Storage)make airflow-shell        # Airflow container

- **Location**: HDFS `/movie-data/bronze/movies/`make spark-master-shell   # Spark master container

- **Format**: Parquet (Snappy compressed)make mongo-shell          # MongoDB shell

- **Partitioning**: `year/month/day/hour````

- **Schema**: Raw JSON + extraction metadata

- **Retention**: 90 days### Verify Pipeline Execution

- **Purpose**: Immutable raw data for reprocessing

1. **Check Airflow DAG**:

#### ğŸ¥ˆ Silver Layer (Clean & Enriched)   - Open http://localhost:8080

- **Location**: HDFS `/movie-data/silver/movies/`   - Login with `admin` / `admin`

- **Format**: Parquet (Snappy compressed)   - Navigate to DAGs â†’ `tmdb_batch_pipeline`

- **Partitioning**: `year/month/genre`   - Unpause the DAG (toggle switch)

- **Schema**: 40+ typed fields with quality metrics   - Click "Trigger DAG" or wait for scheduled run

- **Retention**: 2 years

- **Processing**:2. **Monitor Spark Jobs**:

  - âœ“ Deduplication by `movie_id`   - Open http://localhost:8081

  - âœ“ Data type validation & casting   - Check running applications

  - âœ“ Genre ID â†’ Genre name mapping

  - âœ“ Quality score calculation (0-1)3. **Verify HDFS Data**:

  - âœ“ Null handling & default values   ```bash

  - âœ“ Date extraction (year, month, day)   make hdfs-ls

   # Should show data in /data/bronze, /data/silver, /data/gold

**Key Fields:**   ```

```python

- movie_id (integer, unique)4. **Check MongoDB Data**:

- title, original_title (strings)   ```bash

- release_date, release_year, release_month (date fields)   make mongo-query-views

- vote_average (float, 0-10), vote_count (integer)   # Should show documents in batch_views collection

- popularity (float)   ```

- genres (array), primary_genre (string)

- quality_score (float, 0-1)### Troubleshooting

- is_complete (boolean)

- processed_timestamp (timestamp)**Services not starting?**

``````bash

# Check service health

#### ğŸ¥‡ Gold Layer (Aggregated Analytics)make health

- **Location**: HDFS `/movie-data/gold/{metric_type}/`

- **Format**: Parquet (Snappy compressed)# View logs for failing service

- **Partitioning**: `year/month`make logs

- **Retention**: 5 years

- **Metrics**: 3 types# Restart services

make restart

**1. Genre Analytics** (`/gold/genre_analytics/`)```

```python

{**HDFS safe mode?**

  "genre": "Action",```bash

  "year": 2025,# Wait 30 seconds for HDFS to exit safe mode

  "month": 11,docker compose -f docker-compose.batch.yml exec namenode hdfs dfsadmin -safemode get

  "total_movies": 45,```

  "avg_rating": 7.2,

  "avg_popularity": 68.5,**Airflow DAG not showing?**

  "total_revenue": 2500000000,```bash

  "avg_budget": 120000000,# List DAGs

  "top_movies": ["Movie 1", "Movie 2", ...],  # Top 10make list-dags

  "computed_timestamp": "2025-11-09T10:00:00Z"

}# Check Airflow logs

```make logs-airflow

```

**2. Trending Movies** (`/gold/trending/`)

```python**Out of memory?**

{```bash

  "movie_id": 550,# Check Docker resources

  "title": "Dune Part 3",docker stats

  "window_days": 7,  # or 30

  "trending_score": 98.5,# Reduce Spark worker memory in docker-compose.batch.yml

  "rank": 1,# SPARK_WORKER_MEMORY=2g (instead of 4g)

  "computed_timestamp": "2025-11-09T10:00:00Z"```

}

```---



**3. Temporal Analytics** (`/gold/temporal_analytics/`)## Architecture

```python

{```

  "year": 2025,TMDB API (scheduled extraction)

  "month": 11,    â†“ (Airflow DAG - every 4 hours)

  "total_movies": 243,â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

  "avg_rating": 7.35,â”‚         BRONZE LAYER (HDFS)           â”‚

  "avg_popularity": 55.8,â”‚  â€¢ Raw JSON â†’ Parquet                 â”‚

  "top_genre": "Action",â”‚  â€¢ Partition: /year/month/day/hour    â”‚

  "release_distribution": {"1-10": 78, "11-20": 82, "21-30": 83}â”‚  â€¢ No transformations, immutable      â”‚

}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```                 â†“ (Spark Batch Job)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

### Output Destinationâ”‚         SILVER LAYER (HDFS)           â”‚

â”‚  â€¢ Deduplication (movie_id)           â”‚

**MongoDB** - Serving Layerâ”‚  â€¢ Schema validation & enrichment     â”‚

- **Database**: `moviedb`â”‚  â€¢ Genre/cast joins                   â”‚

- **Collection**: `batch_views`â”‚  â€¢ Historical sentiment analysis      â”‚

- **Connection**: `mongodb://admin:password@localhost:27017/moviedb`â”‚  â€¢ Partition: /year/month/genre       â”‚

- **Update Strategy**: Upsert (insert or replace)â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

- **Indexes**: 6 optimized indexes for fast queries                 â†“ (Spark Aggregations)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

**Document Structure:**â”‚          GOLD LAYER (HDFS)            â”‚

```javascriptâ”‚  â€¢ Aggregations by genre/year/tier    â”‚

{â”‚  â€¢ Trend scores (7d, 30d, 90d)        â”‚

  "_id": ObjectId("..."),â”‚  â€¢ Popularity metrics                 â”‚

  "view_type": "genre_analytics",  // or "trending_movies", "temporal_analytics"â”‚  â€¢ Partition: /metric_type/year/month â”‚

  "batch_run_id": "batch_2025_11_09_10",â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  "computed_at": ISODate("2025-11-09T10:00:00Z"),                 â†“ (Export to Serving)

  "data_freshness": "batch",â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

  "update_frequency": "4_hours",â”‚      MONGODB (Batch Views)            â”‚

  â”‚  â€¢ Collection: batch_views            â”‚

  // Type-specific fieldsâ”‚  â€¢ Updated every 4 hours              â”‚

  "genre": "Action",â”‚  â€¢ Indexed for fast queries           â”‚

  "year": 2025,â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  "month": 11,```

  "total_movies": 45,

  "avg_rating": 7.2,---

  "top_movies": [...],

  ...## Key Characteristics

}

```| Property | Value | Rationale |

|----------|-------|-----------|

**Indexes:**| **Schedule** | Every 4 hours | Balances freshness vs cost |

1. `_id_` - Default primary key| **Accuracy** | 100% | No approximations allowed |

2. `view_type + genre + year + month` - Genre analytics queries| **Latency** | Hours | Acceptable for historical data |

3. `view_type + movie_id` - Movie-specific queries| **Reprocessing** | Full history | Can recompute from scratch |

4. `view_type + computed_at` - Freshness queries| **Storage** | HDFS | Distributed, fault-tolerant |

5. `batch_run_id` - Batch run tracking| **Retention** | Bronze: 90d, Silver: 2y, Gold: 5y | Cost vs compliance |

6. `view_type + window_days + trending_score` - Trending queries

---

---

## Directory Structure

## ğŸ”§ Configuration

```

### Environment Variablesbatch_layer/

â”œâ”€â”€ README.md                    # This file

Key variables in `.env`:â”‚

â”œâ”€â”€ airflow_dags/               # Orchestration

```bashâ”‚   â”œâ”€â”€ __init__.py

# TMDB APIâ”‚   â”œâ”€â”€ batch_ingestion_dag.py  # Bronze layer ingestion

TMDB_API_KEY=your_api_key_here          # Required: Get from TMDBâ”‚   â”œâ”€â”€ batch_transform_dag.py  # Silver layer processing

TMDB_BASE_URL=https://api.themoviedb.org/3â”‚   â”œâ”€â”€ batch_aggregate_dag.py  # Gold layer aggregations

TMDB_RATE_LIMIT=4.0                      # Requests per secondâ”‚   â””â”€â”€ README.md               # DAG documentation

â”‚

# HDFSâ”œâ”€â”€ spark_jobs/                 # Batch processing

HDFS_NAMENODE=hdfs://namenode:8020â”‚   â”œâ”€â”€ __init__.py

HDFS_REPLICATION=3â”‚   â”œâ”€â”€ bronze_to_silver.py     # Cleaning & enrichment

â”‚   â”œâ”€â”€ silver_to_gold.py       # Aggregations

# Sparkâ”‚   â”œâ”€â”€ sentiment_batch.py      # Historical sentiment

SPARK_MASTER_URL=spark://spark-master:7077â”‚   â”œâ”€â”€ actor_networks.py       # Graph analytics

SPARK_EXECUTOR_MEMORY=4gâ”‚   â””â”€â”€ README.md               # Job documentation

SPARK_EXECUTOR_CORES=2â”‚

â”œâ”€â”€ master_dataset/             # Immutable raw data

# MongoDBâ”‚   â”œâ”€â”€ __init__.py

MONGODB_CONNECTION_STRING=mongodb://admin:password@mongo:27017/moviedb?authSource=adminâ”‚   â”œâ”€â”€ ingestion.py            # TMDB â†’ HDFS pipeline

MONGODB_DATABASE=moviedbâ”‚   â”œâ”€â”€ schema.py               # Data schemas

â”‚   â”œâ”€â”€ partitioning.py         # Partition strategies

# Batch Processingâ”‚   â””â”€â”€ README.md               # Dataset documentation

BATCH_INTERVAL_HOURS=4                   # Run every 4 hoursâ”‚

BRONZE_RETENTION_DAYS=90â”œâ”€â”€ batch_views/                # Pre-computed views

SILVER_RETENTION_DAYS=730â”‚   â”œâ”€â”€ __init__.py

GOLD_RETENTION_DAYS=1825â”‚   â”œâ”€â”€ movie_analytics.py      # Movie-level aggregations

â”‚   â”œâ”€â”€ genre_trends.py         # Genre-based analytics

# Airflowâ”‚   â”œâ”€â”€ temporal_analysis.py    # Time-series views

AIRFLOW__CORE__EXECUTOR=LocalExecutorâ”‚   â”œâ”€â”€ export_to_mongo.py      # MongoDB integration

AIRFLOW__CORE__FERNET_KEY=your_key_here  # Generate with cryptography.fernetâ”‚   â””â”€â”€ README.md               # Views documentation

```â”‚

â”œâ”€â”€ config/                     # Configuration

### Resource Requirementsâ”‚   â”œâ”€â”€ spark_config.yaml       # Spark settings

â”‚   â”œâ”€â”€ hdfs_config.yaml        # HDFS settings

**Minimum:**â”‚   â””â”€â”€ airflow_config.yaml     # DAG settings

- CPU: 4 coresâ”‚

- RAM: 8GBâ””â”€â”€ tests/                      # Unit tests

- Disk: 20GB    â”œâ”€â”€ test_transformations.py

    â”œâ”€â”€ test_aggregations.py

**Recommended:**    â””â”€â”€ test_data_quality.py

- CPU: 8 cores```

- RAM: 16GB

- Disk: 50GB+---



---## Data Layers



## ğŸŒ Access Points### Bronze Layer (Raw Data)



Once services are running:**Purpose**: Store immutable, raw data exactly as received from TMDB API



| Service | URL | Credentials | Purpose |**Schema**:

|---------|-----|-------------|---------|```python

| **Airflow UI** | http://localhost:8080 | admin/admin | DAG monitoring & management |# Parquet format

| **Spark Master UI** | http://localhost:8081 | None | Spark cluster status |bronze_schema = {

| **HDFS NameNode UI** | http://localhost:9870 | None | HDFS filesystem browser |    "movie_id": "int",

| **Mongo Express UI** | http://localhost:8082 | admin/password | MongoDB data browser |    "raw_json": "string",  # Complete API response

| **MongoDB** | mongodb://localhost:27017 | admin/password | Direct DB connection |    "api_endpoint": "string",  # movies/people/reviews

    "extraction_timestamp": "timestamp",

---    "partition_year": "int",

    "partition_month": "int",

## ğŸ® Usage    "partition_day": "int",

    "partition_hour": "int"

### View Batch Views Data}

```

```bash

# Display comprehensive batch views report**Partitioning**:

/tmp/show_mongodb_batch_views.sh```

/bronze/movies/year=2025/month=10/day=17/hour=14/

# Output shows:/bronze/reviews/year=2025/month=10/day=17/hour=14/

# - Collection statistics  /bronze/people/year=2025/month=10/day=17/hour=14/

# - Documents by view type```

# - Genre analytics details

# - Trending movies**Retention**: 90 days (configurable)

# - Temporal analytics

# - Data freshness & indexes**Implementation**: `master_dataset/ingestion.py`

```

---

### Manual Pipeline Execution

### Silver Layer (Cleaned Data)

#### Option 1: Run Complete DAG (Recommended)

**Purpose**: Cleaned, validated, and enriched data ready for analytics

```bash

# Unpause the DAG**Transformations**:

sudo docker exec batch_airflow_webserver airflow dags unpause tmdb_batch_pipeline1. **Deduplication**: Remove duplicate movie_id entries

2. **Schema Validation**: Ensure data quality

# Trigger manual run3. **Enrichment**: 

sudo docker exec batch_airflow_webserver airflow dags trigger tmdb_batch_pipeline   - Join genre names from genre_ids

   - Join cast/crew information

# Monitor progress   - Extract keywords and production companies

sudo docker exec batch_airflow_webserver airflow dags list-runs -d tmdb_batch_pipeline4. **Sentiment Analysis**: Historical review sentiment

5. **Data Type Casting**: Proper types for all fields

# View in Airflow UI: http://localhost:8080

```**Schema**:

```python

#### Option 2: Run Individual Stepssilver_schema = {

    "movie_id": "int",

**Note:** Currently, the pipeline scripts have Python 3.9+ syntax but Spark container has Python 3.8. The manual commands below demonstrate the intended workflow.    "title": "string",

    "release_date": "date",

```bash    "genres": "array<string>",

# Step 1: Ingest Bronze (TMDB API â†’ HDFS)    "vote_average": "double",

sudo docker exec batch_airflow_webserver python \    "vote_count": "int",

  /app/layers/batch_layer/master_dataset/ingestion.py \    "popularity": "double",

  --pages 2 \    "budget": "long",

  --categories popular    "revenue": "long",

    "runtime": "int",

# Step 2: Transform Silver (Bronze â†’ Silver)    "cast": "array<struct<name:string, character:string>>",

sudo docker exec batch_airflow_webserver python \    "crew": "array<struct<name:string, job:string>>",

  /app/layers/batch_layer/spark_jobs/silver/run.py \    "sentiment_score": "double",  # -1 to 1

  --execution-date $(date +%Y-%m-%d) \    "sentiment_label": "string",  # positive/neutral/negative

  --data-type movies    "quality_flag": "string",  # OK/WARNING/ERROR

    "processed_timestamp": "timestamp",

# Step 3: Aggregate Gold (Silver â†’ Gold)    "partition_year": "int",

sudo docker exec batch_airflow_webserver python \    "partition_month": "int",

  /app/layers/batch_layer/spark_jobs/gold/run.py \    "partition_genre": "string"

  --execution-date $(date +%Y-%m-%d) \}

  --metric-type genre_analytics```



# Step 4: Export to MongoDB (Gold â†’ MongoDB)**Partitioning**:

sudo docker exec batch_airflow_webserver python \```

  /app/layers/batch_layer/batch_views/export_to_mongo.py \/silver/movies/year=2025/month=10/genre=action/

  --execution-date $(date +%Y-%m-%d) \/silver/reviews/year=2025/month=10/genre=action/

  --metric-type genre_analytics```

```

**Retention**: 2 years

### Query MongoDB Data

**Implementation**: `spark_jobs/bronze_to_silver.py`

```bash

# Using mongosh---

sudo docker exec batch_mongo mongosh moviedb -u admin -p password --authenticationDatabase admin

### Gold Layer (Aggregated Data)

# Sample queries:

**Purpose**: Business-ready aggregations and analytics

# 1. Count documents by view type

db.batch_views.aggregate([**Aggregations**:

  { $group: { _id: "$view_type", count: { $sum: 1 } } }1. **Movie Popularity Trends**: 7-day, 30-day, 90-day rolling windows

])2. **Genre Analytics**: Average ratings, revenue by genre

3. **Temporal Analysis**: Year-over-year comparisons

# 2. Get genre analytics4. **Actor Networks**: Collaboration graphs

db.batch_views.find({ 5. **Sentiment Trends**: Sentiment changes over time

  view_type: "genre_analytics",

  genre: "Action" **Views**:

}).pretty()```python

# Genre aggregations

# 3. Top 5 trending moviesgenre_analytics = {

db.batch_views.find({     "genre": "string",

  view_type: "trending_movies"     "year": "int",

}).sort({     "month": "int",

  trending_score: -1     "total_movies": "int",

}).limit(5)    "avg_rating": "double",

    "total_revenue": "long",

# 4. Temporal analytics for November 2025    "avg_sentiment": "double",

db.batch_views.find({    "top_movies": "array<string>"

  view_type: "temporal_analytics",}

  year: 2025,

  month: 11# Trending scores

})trending_scores = {

```    "movie_id": "int",

    "window": "string",  # 7d/30d/90d

---    "trend_score": "double",

    "velocity": "double",  # rate of popularity change

## ğŸ§ª Testing & Validation    "computed_date": "date"

}

### Run Complete Test Suite```



```bash**Partitioning**:

# Comprehensive end-to-end test```

/tmp/test_batch_layer_complete.sh/gold/genre_analytics/year=2025/month=10/

```/gold/trending_scores/window=7d/year=2025/month=10/

/gold/actor_networks/year=2025/month=10/

**Test Coverage:**```

- âœ… Infrastructure validation (HDFS, Spark, MongoDB, Airflow)

- âœ… Data population with realistic batch views**Retention**: 5 years

- âœ… Data quality checks (counts, types, freshness)

- âœ… Query performance tests**Implementation**: `spark_jobs/silver_to_gold.py`

- âœ… Index validation

---

### Manual Verification

## Airflow DAGs

```bash

# Check service status### 1. Batch Ingestion DAG (`batch_ingestion_dag.py`)

sudo docker compose -f docker-compose.batch.yml ps

**Schedule**: Every 4 hours  

# Check HDFS health**Purpose**: Extract data from TMDB API and store in Bronze layer

sudo docker exec batch_namenode hdfs dfsadmin -report

```python

# Check Spark cluster# DAG Structure

sudo docker exec batch_spark_master /opt/spark/bin/spark-shell --version[start] 

  â†’ [check_api_health]

# Check MongoDB data  â†’ [extract_movies] 

sudo docker exec batch_mongo mongosh moviedb -u admin -p password --authenticationDatabase admin --eval "db.batch_views.countDocuments({})"  â†’ [extract_reviews]

  â†’ [extract_people]

# Check Airflow DAGs  â†’ [validate_extraction]

sudo docker exec batch_airflow_webserver airflow dags list  â†’ [store_to_hdfs_bronze]

```  â†’ [update_metadata]

  â†’ [end]

---```



## ğŸ“ˆ Performance Metrics**Tasks**:

- `check_api_health`: Verify TMDB API availability

### Expected Execution Times- `extract_*`: Parallel extraction from different endpoints

- `validate_extraction`: Check data completeness

| Stage | Duration | Data Volume |- `store_to_hdfs_bronze`: Write Parquet to HDFS

|-------|----------|-------------|- `update_metadata`: Update extraction logs

| Bronze Ingestion (2 pages) | 30-60 sec | ~40 movies, 2-5 MB |

| Silver Transformation | 1-2 min | ~38 movies, 2-4 MB |---

| Gold Aggregation | 1-2 min | ~15-20 records, 100-500 KB |

| MongoDB Export | 10-30 sec | ~15-20 documents, 50-200 KB |### 2. Batch Transform DAG (`batch_transform_dag.py`)

| **Total (Test Run)** | **3-5 min** | **~5-10 MB** |

**Schedule**: 30 minutes after ingestion  

### Full Production Run (20 pages Ã— 3 categories)**Purpose**: Transform Bronze â†’ Silver



| Stage | Duration | Data Volume |```python

|-------|----------|-------------|# DAG Structure

| Bronze Ingestion | 3-5 min | ~1,200 movies, 20-30 MB |[start]

| Silver Transformation | 5-10 min | ~1,140 movies, 18-25 MB |  â†’ [wait_for_bronze]

| Gold Aggregation | 5-10 min | ~50-200 records, 1-2 MB |  â†’ [spark_deduplicate]

| MongoDB Export | 2-5 min | ~50-200 documents, 500 KB-1 MB |  â†’ [spark_validate_schema]

| **Total (Full Run)** | **15-30 min** | **~40-60 MB** |  â†’ [spark_enrich_data]

  â†’ [spark_sentiment_analysis]

### Deduplication Rate  â†’ [write_to_silver]

  â†’ [data_quality_checks]

- **Bronze â†’ Silver**: ~5% duplicates removed  â†’ [end]

- **Unique Keys**: `movie_id` (TMDB unique identifier)```



---**Spark Job**: `spark_jobs/bronze_to_silver.py`



## ğŸ› Troubleshooting---



### Services Won't Start### 3. Batch Aggregate DAG (`batch_aggregate_dag.py`)



```bash**Schedule**: 1 hour after transformation  

# Check Docker resources**Purpose**: Create Gold layer views and export to MongoDB

docker system df

```python

# Clean up if needed# DAG Structure

sudo docker compose -f docker-compose.batch.yml down -v[start]

sudo docker system prune -f  â†’ [wait_for_silver]

  â†’ [spark_genre_aggregations]

# Restart services  â†’ [spark_trending_scores]

sudo docker compose -f docker-compose.batch.yml up -d  â†’ [spark_actor_networks]

```  â†’ [write_to_gold]

  â†’ [export_to_mongodb]

### HDFS Issues  â†’ [update_serving_indexes]

  â†’ [end]

```bash```

# Check HDFS status

sudo docker exec batch_namenode hdfs dfsadmin -report**Spark Jobs**: `spark_jobs/silver_to_gold.py`, `batch_views/export_to_mongo.py`



# List directories---

sudo docker exec batch_namenode hdfs dfs -ls /movie-data

## Spark Jobs

# Check namenode logs

sudo docker logs batch_namenode --tail 100### Bronze to Silver (`spark_jobs/bronze_to_silver.py`)

```

**Input**: HDFS Bronze layer Parquet files  

### Spark Job Failures**Output**: HDFS Silver layer Parquet files



```bash**Processing Steps**:

# Check Spark master logs1. Read Bronze Parquet with pushdown filters

sudo docker logs batch_spark_master --tail 1002. Parse raw JSON and extract fields

3. Deduplicate by movie_id (keep latest)

# Check worker logs4. Validate schema and data types

sudo docker logs batch_spark_worker_1 --tail 1005. Enrich with genre names, cast info

6. Run sentiment analysis on reviews

# View Spark UI for failed jobs7. Add quality flags

# http://localhost:80818. Write to Silver with partitioning

```

**Key Optimizations**:

### MongoDB Connection Issues- Broadcast small lookup tables (genres)

- Partition by year/month/genre

```bash- Use bucketing for frequent joins

# Test MongoDB connection- Z-ordering on movie_id

sudo docker exec batch_mongo mongosh --eval "db.version()"

**Template**: See `spark_jobs/bronze_to_silver.py`

# Check MongoDB logs

sudo docker logs batch_mongo --tail 100---



# Verify credentials### Silver to Gold (`spark_jobs/silver_to_gold.py`)

sudo docker exec batch_mongo mongosh -u admin -p password --authenticationDatabase admin --eval "db.adminCommand('listDatabases')"

```**Input**: HDFS Silver layer Parquet files  

**Output**: HDFS Gold layer aggregated views

### Airflow DAG Not Loading

**Aggregations**:

```bash1. **Genre Analytics**: GROUP BY genre, year, month

# Check for import errors2. **Trending Scores**: Window functions for moving averages

sudo docker exec batch_airflow_webserver airflow dags list-import-errors3. **Actor Networks**: GraphX for collaboration graphs

4. **Temporal Analysis**: Year-over-year comparisons

# Check scheduler logs

sudo docker logs batch_airflow_scheduler --tail 100**Window Functions**:

```python

# Restart scheduler# 7-day rolling average popularity

sudo docker compose -f docker-compose.batch.yml restart airflow-schedulerwindow_7d = Window.partitionBy("movie_id").orderBy("date").rowsBetween(-6, 0)

```df = df.withColumn("popularity_7d_avg", avg("popularity").over(window_7d))

```

### Python Version Compatibility

**Template**: See `spark_jobs/silver_to_gold.py`

**Known Issue:** The current pipeline scripts use Python 3.9+ type hints (`tuple[DataFrame, dict]`) but the Spark container has Python 3.8.

---

**Workaround Options:**

1. Update Spark Docker image to use Python 3.9+## Batch Views (MongoDB Export)

2. Modify type hints to be Python 3.8 compatible: `from typing import Tuple; Tuple[DataFrame, dict]`

3. Use `SparkSubmitOperator` in Airflow instead of direct Python execution### Collections in MongoDB



---```javascript

// batch_views collection

## ğŸ›‘ Stopping Services{

  "_id": ObjectId("..."),

```bash  "movie_id": 12345,

# Stop all services  "view_type": "genre_analytics",  // genre/trending/temporal

sudo docker compose -f docker-compose.batch.yml stop  "data": {

    "genre": "Action",

# Stop and remove containers (keeps volumes)    "year": 2025,

sudo docker compose -f docker-compose.batch.yml down    "month": 10,

    "avg_rating": 7.5,

# Stop and remove everything including data volumes    "total_movies": 150,

sudo docker compose -f docker-compose.batch.yml down -v    "total_revenue": 5000000000

```  },

  "computed_at": ISODate("2025-10-17T14:00:00Z"),

---  "batch_run_id": "batch_2025_10_17_14"

}

## ğŸ“ Project Structure```



```**Indexes**:

batch_layer/```javascript

â”œâ”€â”€ docker-compose.batch.yml      # Service orchestrationdb.batch_views.createIndex({ "movie_id": 1, "view_type": 1 })

â”œâ”€â”€ .env                          # Environment configurationdb.batch_views.createIndex({ "view_type": 1, "data.genre": 1, "data.year": 1 })

â”œâ”€â”€ .env.example                  # Environment templatedb.batch_views.createIndex({ "computed_at": -1 })

â”œâ”€â”€ README.md                     # This file```

â”‚

â”œâ”€â”€ dockerfiles/**Export Process**:

â”‚   â”œâ”€â”€ spark.Dockerfile          # Spark + PySpark image1. Read Gold layer aggregations

â”‚   â””â”€â”€ airflow.Dockerfile        # Airflow + dependencies2. Transform to MongoDB document format

â”‚3. Bulk upsert to `batch_views` collection

â”œâ”€â”€ master_dataset/4. Update indexes

â”‚   â””â”€â”€ ingestion.py              # Bronze layer ingestion (TMDB API)5. Log export metrics

â”‚

â”œâ”€â”€ spark_jobs/**Template**: See `batch_views/export_to_mongo.py`

â”‚   â”œâ”€â”€ bronze/

â”‚   â”‚   â””â”€â”€ run.py                # Bronze layer processing (future)---

â”‚   â”œâ”€â”€ silver/

â”‚   â”‚   â””â”€â”€ run.py                # Silver layer transformation## Docker Infrastructure

â”‚   â”œâ”€â”€ gold/

â”‚   â”‚   â””â”€â”€ run.py                # Gold layer aggregation### Container Architecture

â”‚   â””â”€â”€ utils/

â”‚       â”œâ”€â”€ spark_session.py      # Spark session managementThe batch layer runs in a containerized environment with the following services:

â”‚       â”œâ”€â”€ logging_utils.py      # Logging & metrics

â”‚       â””â”€â”€ transformations.py    # Data transformations#### HDFS Cluster

â”‚- **namenode**: 1 instance (9870: Web UI, 8020: FS port)

â”œâ”€â”€ batch_views/- **datanode**: 3 instances (replicated storage)

â”‚   â””â”€â”€ export_to_mongo.py        # MongoDB export- **Image**: `bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8`

â”‚- **Volumes**: Persistent volumes for each datanode

â””â”€â”€ tests/- **Replication**: Factor of 3 for data durability

    â””â”€â”€ ...                       # Unit & integration tests

```#### Spark Cluster

- **spark-master**: 1 instance (8081: Web UI, 7077: Master port)

---- **spark-worker**: 2 instances (4g memory, 2 cores each)

- **Base Image**: `apache/spark:3.5.0-python3`

## ğŸ“š Documentation- **Custom Build**: Includes Python dependencies and project code

- **Dockerfile**: `docker/spark.Dockerfile`

Additional documentation:

#### Airflow Services

- **[INPUT_OUTPUT_DOCUMENTATION.md](./INPUT_OUTPUT_DOCUMENTATION.md)** - Complete data schemas for all layers- **postgres**: PostgreSQL 15 (Airflow metadata DB)

- **[TESTING_GUIDE.md](./TESTING_GUIDE.md)** - Step-by-step testing procedures- **airflow-init**: Initializes DB, creates users, sets up connections

- **[OUTPUT_VALIDATION_GUIDE.md](./OUTPUT_VALIDATION_GUIDE.md)** - Expected outputs and validation- **airflow-webserver**: Web UI on port 8080

- **airflow-scheduler**: DAG scheduler

---- **Base Image**: `apache/airflow:2.8.0-python3.11`

- **Custom Build**: Includes batch layer dependencies

## ğŸ”„ Scheduled Runs- **Dockerfile**: `docker/airflow.Dockerfile`



The DAG is configured to run **every 4 hours**:#### MongoDB Services

- Schedule: `0 */4 * * *` (at minute 0 of every 4th hour)- **mongo**: MongoDB 7.0 (port 27017)

- Execution times: 00:00, 04:00, 08:00, 12:00, 16:00, 20:00 UTC- **mongo-express**: Web UI on port 8082

- **Initialization**: Auto-creates batch_views collection with indexes

To enable automatic scheduling:- **Script**: `docker/mongo-init.js`

```bash

# Unpause the DAG in Airflow UI or:### Volume Management

sudo docker exec batch_airflow_webserver airflow dags unpause tmdb_batch_pipeline

``````yaml

volumes:

---  namenode_data:      # HDFS namenode metadata

  datanode1_data:     # HDFS datanode 1 blocks

## ğŸ’¡ Tips  datanode2_data:     # HDFS datanode 2 blocks

  datanode3_data:     # HDFS datanode 3 blocks

1. **Monitor Disk Space**: HDFS data grows over time. Monitor with `sudo docker exec batch_namenode hdfs dfs -df -h`  airflow_dags:       # Airflow DAG definitions

2. **Check Data Freshness**: MongoDB data should be < 4 hours old. Check with `/tmp/show_mongodb_batch_views.sh`  airflow_logs:       # Airflow execution logs

3. **Resource Limits**: Adjust Spark executor memory in `.env` if you have more/less RAM  airflow_plugins:    # Airflow plugins

4. **API Rate Limits**: Stay within TMDB's 4 req/s limit by not running multiple ingestions simultaneously  postgres_data:      # Airflow metadata

5. **Backup MongoDB**: Export batch views periodically: `mongodump --db moviedb --collection batch_views`  mongo_data:         # MongoDB batch_views data

```

---

### Network Configuration

## ğŸ“ Next Steps

All services run on a bridge network `movie_batch_net`:

After batch layer is running:- DNS resolution between containers by service name

- Internal communication (HDFS, Spark, MongoDB)

1. **Integrate Speed Layer**: Add Kafka streaming for real-time updates- Exposed ports for external access (UIs, APIs)

2. **Build Serving Layer API**: Create REST API on top of MongoDB batch views

3. **Add Data Quality Monitoring**: Implement Great Expectations for data validation### Environment Variables

4. **Set Up Alerts**: Configure Airflow email/Slack alerts for failures

5. **Optimize Performance**: Tune Spark partitions and MongoDB indexes based on query patternsCore environment variables (defined in `.env`):



---```bash

# TMDB API

## âœ… Verification SummaryTMDB_API_KEY=36bdc639ae379da0a89bfb9c556e2136

TMDB_BASE_URL=https://api.themoviedb.org/3

**Current Status (as of November 9, 2025):**TMDB_RATE_LIMIT=4.0



- âœ… **Infrastructure**: All 11 services running and healthy# HDFS

- âœ… **MongoDB**: `moviedb.batch_views` collection with 38 documentsHDFS_NAMENODE=hdfs://namenode:8020

- âœ… **Data Quality**: HDFS_REPLICATION=3

  - 8 genres (Action, Drama, Comedy, Thriller, Sci-Fi, Horror, Romance, Animation)

  - 10 trending movies with scores 86-99# Spark

  - 1 temporal analytics record for Nov 2025SPARK_MASTER_URL=spark://spark-master:7077

- âœ… **Indexes**: 6 optimized indexes createdSPARK_EXECUTOR_MEMORY=4g

- âœ… **Queries**: All query types tested and workingSPARK_DRIVER_MEMORY=2g

- âœ… **Test Suite**: 13/13 tests passed

# MongoDB

**Quick Verification:**MONGODB_CONNECTION_STRING=mongodb://admin:password@mongo:27017/moviedb?authSource=admin

```bash

# Run: /tmp/test_batch_layer_complete.sh# Batch Config

# Expected: All tests pass, 38 documents in MongoDBBATCH_INTERVAL_HOURS=4

``````



---### Health Checks



**Questions or Issues?** Check troubleshooting section or review logs in service UIs.All critical services have health checks:



**Happy Data Processing! ğŸš€**- **namenode**: `curl http://localhost:9870`

- **spark-master**: `curl http://localhost:8080`
- **airflow-webserver**: `curl http://localhost:8080/health`
- **mongo**: `mongosh --eval "db.adminCommand('ping')"`
- **postgres**: `pg_isready -U airflow`

### Initialization Sequence

1. **postgres** starts first (Airflow metadata DB)
2. **namenode** initializes HDFS cluster
3. **datanode** instances join namenode
4. **spark-master** starts
5. **spark-worker** instances join master
6. **mongo** starts and runs `mongo-init.js`
7. **airflow-init** sets up DB, users, connections
8. **airflow-webserver** and **airflow-scheduler** start
9. **mongo-express** provides UI for MongoDB

### Resource Requirements

Minimum system requirements:
- **CPU**: 4 cores
- **RAM**: 8GB (16GB recommended)
- **Disk**: 20GB free space
- **Docker**: 24.0+
- **Docker Compose**: 2.0+

### Dockerfile Details

#### Spark Dockerfile (`docker/spark.Dockerfile`)
```dockerfile
FROM apache/spark:3.5.0-python3
# Install system dependencies
# Install Python packages (pyspark, pymongo, great-expectations)
# Copy project code
# Set PYTHONPATH and PYSPARK_PYTHON
```

#### Airflow Dockerfile (`docker/airflow.Dockerfile`)
```dockerfile
FROM apache/airflow:2.8.0-python3.11
# Install system dependencies
# Install Python packages (airflow, pyspark, pymongo)
# Copy project code and DAGs
# Set AIRFLOW_HOME and PYTHONPATH
```

### Makefile Commands

The `Makefile` provides convenient commands for managing the stack:

| Command | Description |
|---------|-------------|
| `make up` | Start all services |
| `make down` | Stop services (keep volumes) |
| `make down-v` | Stop services and remove volumes |
| `make logs` | Follow all logs |
| `make logs-airflow` | Follow Airflow logs only |
| `make logs-spark` | Follow Spark logs only |
| `make ps` | Show service status |
| `make health` | Check health of all services |
| `make ui` | Display service URLs |
| `make trigger-dag` | Trigger the batch pipeline DAG |
| `make hdfs-ls` | List HDFS directories |
| `make hdfs-create-dirs` | Create required HDFS paths |
| `make mongo-query-views` | Query batch_views collection |
| `make backfill START=... END=...` | Run backfill for date range |
| `make airflow-shell` | Open Airflow container shell |
| `make spark-master-shell` | Open Spark master shell |
| `make mongo-shell` | Open MongoDB shell |

### Development Workflow

1. **Start services**: `make up`
2. **Check health**: `make health`
3. **Create HDFS dirs**: `make hdfs-create-dirs`
4. **Trigger pipeline**: `make trigger-dag`
5. **Monitor execution**:
   - Airflow UI: http://localhost:8080
   - Spark UI: http://localhost:8081
   - HDFS UI: http://localhost:9870
6. **Verify results**: `make mongo-query-views`
7. **Check data**: `make hdfs-ls`
8. **View logs**: `make logs-airflow`

### Cleanup and Reset

```bash
# Stop services but keep data
make down

# Stop services and remove all data
make down-v

# Clean up Docker resources
make clean

# Rebuild images from scratch
make build
```

---

## Configuration

### Spark Configuration (`config/spark_config.yaml`)

```yaml
spark:
  app_name: "batch_layer_processing"
  master: "spark://spark-master:7077"
  
  executor:
    memory: "4g"
    cores: 2
    instances: 10
  
  driver:
    memory: "2g"
    cores: 1
  
  sql:
    adaptive_enabled: true
    coalesce_partitions_enabled: true
    broadcast_timeout: 300
  
  serializer: "org.apache.spark.serializer.KryoSerializer"
```

### HDFS Configuration (`config/hdfs_config.yaml`)

```yaml
hdfs:
  namenode: "hdfs://namenode:9000"
  replication_factor: 3
  block_size: "128m"
  
  paths:
    bronze: "/data/bronze"
    silver: "/data/silver"
    gold: "/data/gold"
  
  retention:
    bronze_days: 90
    silver_days: 730
    gold_days: 1825
```

---

## Next Phase Implementation Tasks

### Phase 2A: Master Dataset Ingestion (Week 3)
- [ ] Implement `master_dataset/ingestion.py`
  - TMDB API client with rate limiting
  - Batch extraction for movies, reviews, people
  - Write raw JSON to HDFS Bronze layer
  - Implement retry logic and error handling

- [ ] Create `master_dataset/schema.py`
  - Define Bronze layer Parquet schemas
  - Validation rules
  - Partition strategies

- [ ] Build `airflow_dags/batch_ingestion_dag.py`
  - 4-hour schedule
  - Parallel extraction tasks
  - Health checks and monitoring
  - Slack/email alerts

### Phase 2B: Silver Layer Transformations (Week 4)
- [ ] Implement `spark_jobs/bronze_to_silver.py`
  - Read Bronze Parquet
  - Deduplication logic
  - Schema validation
  - Data enrichment (genres, cast)

- [ ] Create `spark_jobs/sentiment_batch.py`
  - Historical sentiment analysis
  - VADER or transformer model
  - Batch processing of reviews

- [ ] Build `airflow_dags/batch_transform_dag.py`
  - Trigger after ingestion
  - Spark job orchestration
  - Data quality checks

### Phase 2C: Gold Layer Aggregations (Week 5)
- [ ] Implement `spark_jobs/silver_to_gold.py`
  - Genre aggregations
  - Trending scores with window functions
  - Temporal analysis

- [ ] Create `spark_jobs/actor_networks.py`
  - GraphX-based network analysis
  - Collaboration strength calculations
  - Community detection

- [ ] Build `batch_views/export_to_mongo.py`
  - Gold â†’ MongoDB pipeline
  - Bulk upsert logic
  - Index maintenance

- [ ] Create `airflow_dags/batch_aggregate_dag.py`
  - Trigger after transformations
  - Gold layer processing
  - MongoDB export

### Phase 2D: Testing & Monitoring (Week 5)
- [ ] Write unit tests
  - Test deduplication logic
  - Test aggregation accuracy
  - Test MongoDB export

- [ ] Set up data quality checks
  - Row count validation
  - Schema validation
  - Completeness checks

- [ ] Configure monitoring
  - DAG success/failure rates
  - Spark job duration
  - HDFS storage utilization

---

## Testing Strategy

### Unit Tests
```python
# tests/test_transformations.py
def test_deduplication():
    # Create sample DataFrame with duplicates
    # Run deduplication
    # Assert no duplicates remain
    pass

def test_sentiment_analysis():
    # Sample reviews with known sentiments
    # Run sentiment analysis
    # Assert accuracy > 80%
    pass
```

### Integration Tests
```python
# tests/test_end_to_end.py
def test_bronze_to_silver_pipeline():
    # Write sample Bronze data
    # Run Spark job
    # Validate Silver output
    pass
```

### Data Quality Tests
```python
# tests/test_data_quality.py
def test_completeness():
    # Check for null values in required fields
    pass

def test_schema_compliance():
    # Validate schema matches expected
    pass
```

---

## Monitoring & Alerts

### Key Metrics
- **Batch Job Success Rate**: Target > 99%
- **Processing Time**: < 2 hours per batch
- **Data Quality Score**: > 95% rows passing validation
- **HDFS Storage Growth**: Track and predict capacity

### Alerts
- DAG failure â†’ Slack channel + email
- Data quality below threshold â†’ PagerDuty
- HDFS capacity > 80% â†’ Ops team notification
- Job duration > 3 hours â†’ Warning alert

---

## Troubleshooting

### Common Issues

**Issue**: Spark job OOM errors  
**Solution**: Increase executor memory, reduce partition size

**Issue**: HDFS write failures  
**Solution**: Check namenode health, verify replication factor

**Issue**: Duplicate records in Silver  
**Solution**: Review deduplication logic, check for partition key issues

**Issue**: Slow MongoDB export  
**Solution**: Use bulk writes, create appropriate indexes

---

## References

- [Apache Spark Best Practices](https://spark.apache.org/docs/latest/sql-performance-tuning.html)
- [HDFS Architecture](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)
- [Airflow DAG Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [Lambda Architecture - Batch Layer](http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html)

---

**Next Step**: Proceed to [Speed Layer README](../speed_layer/README.md)
