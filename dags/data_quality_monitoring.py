"""
Data quality monitoring DAG.
This DAG monitors data quality, validates schemas, and ensures data integrity.
"""
from datetime import datetime, timedelta
from typing import Dict, Any, List

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.email_operator import EmailOperator
from airflow.models import Variable
from airflow.utils.dates import days_ago

import sys
import os

# Add project root to Python path
sys.path.append('/opt/airflow')

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(minutes=30),
}

dag = DAG(
    'data_quality_monitoring',
    default_args=default_args,
    description='Data quality monitoring and validation pipeline',
    schedule_interval='0 */6 * * *',  # Run every 6 hours
    catchup=False,
    max_active_runs=1,
    tags=['monitoring', 'quality', 'validation'],
)

def validate_data_schema(**context) -> Dict[str, Any]:
    """Validate data schema in MongoDB collections."""
    from pymongo import MongoClient
    import logging
    
    logger = logging.getLogger(__name__)
    
    try:
        mongodb_uri = os.getenv('MONGODB_URI')
        client = MongoClient(mongodb_uri)
        db = client.moviedb
        
        validation_results = {}\n        \n        # Define expected schemas\n        movie_schema = {\n            'required_fields': ['id', 'title', 'overview'],\n            'optional_fields': ['release_date', 'vote_average', 'popularity', 'genres']\n        }\n        \n        review_schema = {\n            'required_fields': ['id', 'content', 'movie_id'],\n            'optional_fields': ['author', 'created_at', 'rating']\n        }\n        \n        # Validate movies collection\n        movies_sample = list(db.processed_movies.aggregate([\n            {'$sample': {'size': 100}}\n        ]))\n        \n        movie_validation = validate_collection_schema(movies_sample, movie_schema)\n        validation_results['movies'] = movie_validation\n        \n        # Validate sentiment analysis collection\n        sentiment_sample = list(db.sentiment_analysis.aggregate([\n            {'$sample': {'size': 100}}\n        ]))\n        \n        sentiment_validation = validate_collection_schema(sentiment_sample, review_schema)\n        validation_results['sentiment_analysis'] = sentiment_validation\n        \n        client.close()\n        \n        # Calculate overall validation score\n        total_score = sum([result['score'] for result in validation_results.values()])\n        avg_score = total_score / len(validation_results) if validation_results else 0\n        \n        logger.info(f"Schema validation completed. Average score: {avg_score:.2f}")\n        \n        return {\n            'validation_results': validation_results,\n            'average_score': avg_score,\n            'validation_time': datetime.now().isoformat(),\n            'status': 'success' if avg_score > 0.8 else 'warning'\n        }\n        \n    except Exception as e:\n        logger.error(f"Error during schema validation: {e}")\n        raise\n\ndef validate_collection_schema(documents: List[Dict], schema: Dict) -> Dict[str, Any]:\n    """Validate a collection against a schema."""\n    if not documents:\n        return {'score': 0, 'issues': ['No documents found'], 'total_docs': 0}\n    \n    required_fields = schema['required_fields']\n    issues = []\n    valid_docs = 0\n    \n    for doc in documents:\n        doc_valid = True\n        for field in required_fields:\n            if field not in doc or doc[field] is None:\n                issues.append(f"Missing required field '{field}' in document {doc.get('_id', 'unknown')}")\n                doc_valid = False\n        \n        if doc_valid:\n            valid_docs += 1\n    \n    score = valid_docs / len(documents) if documents else 0\n    \n    return {\n        'score': score,\n        'valid_documents': valid_docs,\n        'total_documents': len(documents),\n        'issues': issues[:10]  # Limit to first 10 issues\n    }\n\ndef check_data_freshness(**context) -> Dict[str, Any]:\n    """Check if data is fresh and up-to-date."""\n    from pymongo import MongoClient\n    import logging\n    \n    logger = logging.getLogger(__name__)\n    \n    try:\n        mongodb_uri = os.getenv('MONGODB_URI')\n        client = MongoClient(mongodb_uri)\n        db = client.moviedb\n        \n        freshness_results = {}\n        \n        # Check when data was last updated\n        collections_to_check = ['processed_movies', 'sentiment_analysis', 'analytics']\n        \n        for collection_name in collections_to_check:\n            collection = db[collection_name]\n            \n            # Find latest document\n            latest_doc = collection.find().sort([('_id', -1)]).limit(1)\n            latest_doc = list(latest_doc)\n            \n            if latest_doc:\n                # Extract timestamp from ObjectId\n                latest_timestamp = latest_doc[0]['_id'].generation_time\n                age_hours = (datetime.now(latest_timestamp.tzinfo) - latest_timestamp).total_seconds() / 3600\n                \n                freshness_results[collection_name] = {\n                    'latest_update': latest_timestamp.isoformat(),\n                    'age_hours': age_hours,\n                    'is_fresh': age_hours < 24  # Consider fresh if less than 24 hours old\n                }\n            else:\n                freshness_results[collection_name] = {\n                    'latest_update': None,\n                    'age_hours': float('inf'),\n                    'is_fresh': False\n                }\n        \n        client.close()\n        \n        # Calculate overall freshness\n        fresh_collections = sum([1 for result in freshness_results.values() if result['is_fresh']])\n        freshness_score = fresh_collections / len(freshness_results) if freshness_results else 0\n        \n        logger.info(f"Data freshness check completed. Score: {freshness_score:.2f}")\n        \n        return {\n            'freshness_results': freshness_results,\n            'freshness_score': freshness_score,\n            'check_time': datetime.now().isoformat(),\n            'status': 'success' if freshness_score > 0.8 else 'warning'\n        }\n        \n    except Exception as e:\n        logger.error(f"Error during freshness check: {e}")\n        raise\n\ndef check_data_volume(**context) -> Dict[str, Any]:\n    """Check data volume and detect anomalies."""\n    from pymongo import MongoClient\n    import logging\n    \n    logger = logging.getLogger(__name__)\n    \n    try:\n        mongodb_uri = os.getenv('MONGODB_URI')\n        client = MongoClient(mongodb_uri)\n        db = client.moviedb\n        \n        volume_results = {}\n        \n        # Check document counts\n        collections_to_check = ['processed_movies', 'sentiment_analysis', 'analytics']\n        \n        for collection_name in collections_to_check:\n            collection = db[collection_name]\n            \n            # Get current count\n            current_count = collection.count_documents({})\n            \n            # Get count from 24 hours ago\n            yesterday = datetime.now() - timedelta(days=1)\n            yesterday_count = collection.count_documents({\n                '_id': {'$lt': yesterday}\n            })\n            \n            # Calculate growth\n            growth = current_count - yesterday_count\n            growth_rate = (growth / yesterday_count * 100) if yesterday_count > 0 else float('inf')\n            \n            volume_results[collection_name] = {\n                'current_count': current_count,\n                'yesterday_count': yesterday_count,\n                'growth': growth,\n                'growth_rate_percent': growth_rate,\n                'is_normal': -50 <= growth_rate <= 1000  # Normal range: -50% to 1000%\n            }\n        \n        client.close()\n        \n        # Calculate overall volume health\n        normal_collections = sum([1 for result in volume_results.values() if result['is_normal']])\n        volume_score = normal_collections / len(volume_results) if volume_results else 0\n        \n        logger.info(f"Data volume check completed. Score: {volume_score:.2f}")\n        \n        return {\n            'volume_results': volume_results,\n            'volume_score': volume_score,\n            'check_time': datetime.now().isoformat(),\n            'status': 'success' if volume_score > 0.8 else 'warning'\n        }\n        \n    except Exception as e:\n        logger.error(f"Error during volume check: {e}")\n        raise\n\ndef check_kafka_lag(**context) -> Dict[str, Any]:\n    """Check Kafka consumer lag."""\n    from kafka import KafkaConsumer\n    from kafka.structs import TopicPartition\n    import logging\n    \n    logger = logging.getLogger(__name__)\n    \n    try:\n        kafka_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka:29092')\n        \n        consumer = KafkaConsumer(\n            bootstrap_servers=[kafka_servers],\n            enable_auto_commit=False,\n            group_id='monitoring_group'\n        )\n        \n        lag_results = {}\n        topics = ['movies', 'people', 'credits', 'reviews']\n        \n        for topic in topics:\n            try:\n                # Get topic partitions\n                partitions = consumer.partitions_for_topic(topic)\n                if not partitions:\n                    lag_results[topic] = {'status': 'topic_not_found'}\n                    continue\n                \n                topic_partitions = [TopicPartition(topic, p) for p in partitions]\n                \n                # Get latest offsets\n                latest_offsets = consumer.end_offsets(topic_partitions)\n                \n                # Calculate total messages\n                total_messages = sum(latest_offsets.values())\n                \n                lag_results[topic] = {\n                    'partitions': len(partitions),\n                    'total_messages': total_messages,\n                    'status': 'healthy' if total_messages >= 0 else 'error'\n                }\n                \n            except Exception as e:\n                lag_results[topic] = {\n                    'status': 'error',\n                    'error': str(e)\n                }\n        \n        consumer.close()\n        \n        # Calculate overall Kafka health\n        healthy_topics = sum([1 for result in lag_results.values() if result.get('status') == 'healthy'])\n        kafka_score = healthy_topics / len(topics) if topics else 0\n        \n        logger.info(f"Kafka lag check completed. Score: {kafka_score:.2f}")\n        \n        return {\n            'lag_results': lag_results,\n            'kafka_score': kafka_score,\n            'check_time': datetime.now().isoformat(),\n            'status': 'success' if kafka_score > 0.8 else 'warning'\n        }\n        \n    except Exception as e:\n        logger.error(f"Error during Kafka lag check: {e}")\n        raise\n\ndef generate_quality_report(**context) -> Dict[str, Any]:\n    """Generate comprehensive data quality report."""\n    import logging\n    \n    logger = logging.getLogger(__name__)\n    \n    # Collect results from all quality checks\n    schema_results = context['task_instance'].xcom_pull(task_ids='validate_data_schema') or {}\n    freshness_results = context['task_instance'].xcom_pull(task_ids='check_data_freshness') or {}\n    volume_results = context['task_instance'].xcom_pull(task_ids='check_data_volume') or {}\n    kafka_results = context['task_instance'].xcom_pull(task_ids='check_kafka_lag') or {}\n    \n    # Calculate overall quality score\n    scores = [\n        schema_results.get('average_score', 0),\n        freshness_results.get('freshness_score', 0),\n        volume_results.get('volume_score', 0),\n        kafka_results.get('kafka_score', 0)\n    ]\n    \n    overall_score = sum(scores) / len(scores) if scores else 0\n    \n    # Determine overall status\n    if overall_score >= 0.9:\n        overall_status = 'excellent'\n    elif overall_score >= 0.8:\n        overall_status = 'good'\n    elif overall_score >= 0.6:\n        overall_status = 'warning'\n    else:\n        overall_status = 'critical'\n    \n    quality_report = {\n        'overall_score': overall_score,\n        'overall_status': overall_status,\n        'schema_validation': schema_results,\n        'data_freshness': freshness_results,\n        'data_volume': volume_results,\n        'kafka_health': kafka_results,\n        'report_time': datetime.now().isoformat(),\n        'dag_run_id': context['dag_run'].run_id\n    }\n    \n    # Save report to MongoDB\n    try:\n        from pymongo import MongoClient\n        \n        mongodb_uri = os.getenv('MONGODB_URI')\n        client = MongoClient(mongodb_uri)\n        db = client.moviedb\n        \n        db.quality_reports.insert_one(quality_report)\n        client.close()\n        \n        logger.info(f"Quality report generated. Overall score: {overall_score:.2f}, Status: {overall_status}")\n        \n    except Exception as e:\n        logger.error(f"Error saving quality report: {e}")\n    \n    return quality_report\n\ndef send_quality_alert(**context) -> None:\n    """Send alert if data quality is below threshold."""\n    import logging\n    \n    logger = logging.getLogger(__name__)\n    \n    quality_report = context['task_instance'].xcom_pull(task_ids='generate_quality_report')\n    \n    if not quality_report:\n        logger.warning("No quality report found")\n        return\n    \n    overall_score = quality_report.get('overall_score', 0)\n    overall_status = quality_report.get('overall_status', 'unknown')\n    \n    # Send alert if quality is below threshold\n    if overall_score < 0.8:  # Alert threshold\n        alert_message = f\"\"\"\n        Data Quality Alert - {overall_status.upper()}\n        \n        Overall Quality Score: {overall_score:.2f}\n        \n        Issues detected:\n        - Schema Validation: {quality_report.get('schema_validation', {}).get('average_score', 0):.2f}\n        - Data Freshness: {quality_report.get('data_freshness', {}).get('freshness_score', 0):.2f}\n        - Data Volume: {quality_report.get('data_volume', {}).get('volume_score', 0):.2f}\n        - Kafka Health: {quality_report.get('kafka_health', {}).get('kafka_score', 0):.2f}\n        \n        Please investigate and take corrective action.\n        \n        Report Time: {quality_report.get('report_time')}\n        DAG Run ID: {quality_report.get('dag_run_id')}\n        \"\"\"\n        \n        logger.warning(f"Data quality alert triggered: {alert_message}")\n        \n        # TODO: Send actual email/Slack notification\n        # For now, just log the alert\n    else:\n        logger.info(f"Data quality is healthy. Score: {overall_score:.2f}")\n\n# Task definitions\nstart_task = DummyOperator(\n    task_id='start_quality_monitoring',\n    dag=dag,\n)\n\n# Quality check tasks\nvalidate_schema_task = PythonOperator(\n    task_id='validate_data_schema',\n    python_callable=validate_data_schema,\n    dag=dag,\n)\n\ncheck_freshness_task = PythonOperator(\n    task_id='check_data_freshness',\n    python_callable=check_data_freshness,\n    dag=dag,\n)\n\ncheck_volume_task = PythonOperator(\n    task_id='check_data_volume',\n    python_callable=check_data_volume,\n    dag=dag,\n)\n\ncheck_kafka_lag_task = PythonOperator(\n    task_id='check_kafka_lag',\n    python_callable=check_kafka_lag,\n    dag=dag,\n)\n\n# Report generation\ngenerate_report_task = PythonOperator(\n    task_id='generate_quality_report',\n    python_callable=generate_quality_report,\n    dag=dag,\n)\n\n# Alert task\nsend_alert_task = PythonOperator(\n    task_id='send_quality_alert',\n    python_callable=send_quality_alert,\n    dag=dag,\n)\n\nend_task = DummyOperator(\n    task_id='end_quality_monitoring',\n    dag=dag,\n)\n\n# Task dependencies\nstart_task >> [\n    validate_schema_task,\n    check_freshness_task,\n    check_volume_task,\n    check_kafka_lag_task\n]\n\n[\n    validate_schema_task,\n    check_freshness_task,\n    check_volume_task,\n    check_kafka_lag_task\n] >> generate_report_task\n\ngenerate_report_task >> send_alert_task >> end_task