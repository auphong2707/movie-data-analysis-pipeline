version: '3.8'

# ========================================
# TMDB Batch Layer - Docker Compose
# ========================================
# Services: HDFS, Spark, Airflow, MongoDB
# Network: movie_batch_net
# Ports: 9870 (HDFS), 8081 (Spark), 8080 (Airflow), 27017 (Mongo), 8082 (Mongo Express)
# ========================================

networks:
  movie_batch_net:
    driver: bridge

volumes:
  # HDFS volumes
  namenode_data:
  datanode1_data:
  datanode2_data:
  datanode3_data:
  
  # Airflow volumes
  airflow_dags:
  airflow_logs:
  airflow_plugins:
  postgres_data:
  
  # MongoDB volume
  mongo_data:

services:
  # ========================================
  # HDFS Services
  # ========================================
  
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: batch_namenode
    hostname: namenode
    networks:
      - movie_batch_net
    ports:
      - "9870:9870"  # Web UI
      - "8020:8020"  # FS port
    volumes:
      - namenode_data:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=batch_cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_replication=3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: batch_datanode1
    hostname: datanode1
    networks:
      - movie_batch_net
    volumes:
      - datanode1_data:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
    depends_on:
      namenode:
        condition: service_healthy

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: batch_datanode2
    hostname: datanode2
    networks:
      - movie_batch_net
    volumes:
      - datanode2_data:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
    depends_on:
      namenode:
        condition: service_healthy

  datanode3:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: batch_datanode3
    hostname: datanode3
    networks:
      - movie_batch_net
    volumes:
      - datanode3_data:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
    depends_on:
      namenode:
        condition: service_healthy

  # ========================================
  # Spark Services
  # ========================================
  
  spark-master:
    build:
      context: ../../
      dockerfile: layers/batch_layer/docker/spark.Dockerfile
    container_name: batch_spark_master
    hostname: spark-master
    networks:
      - movie_batch_net
    ports:
      - "8081:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master port
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - HDFS_NAMENODE=hdfs://namenode:8020
      - TMDB_API_KEY=${TMDB_API_KEY}
      - MONGODB_CONNECTION_STRING=mongodb://admin:password@mongo:27017/moviedb?authSource=admin
    volumes:
      - ../../:/app
      - ../../layers/batch_layer/ge_expectations:/app/ge_expectations
    env_file:
      - ../../.env
      - .env
    depends_on:
      namenode:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    command: >
      bash -c "
        /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      "

  spark-worker-1:
    build:
      context: ../../
      dockerfile: layers/batch_layer/docker/spark.Dockerfile
    container_name: batch_spark_worker_1
    hostname: spark-worker-1
    networks:
      - movie_batch_net
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=4g
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - HDFS_NAMENODE=hdfs://namenode:8020
      - TMDB_API_KEY=${TMDB_API_KEY}
      - MONGODB_CONNECTION_STRING=mongodb://admin:password@mongo:27017/moviedb?authSource=admin
    volumes:
      - ../../:/app
    env_file:
      - ../../.env
      - .env
    depends_on:
      spark-master:
        condition: service_healthy
    command: >
      bash -c "
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
      "

  spark-worker-2:
    build:
      context: ../../
      dockerfile: layers/batch_layer/docker/spark.Dockerfile
    container_name: batch_spark_worker_2
    hostname: spark-worker-2
    networks:
      - movie_batch_net
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=4g
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - HDFS_NAMENODE=hdfs://namenode:8020
      - TMDB_API_KEY=${TMDB_API_KEY}
      - MONGODB_CONNECTION_STRING=mongodb://admin:password@mongo:27017/moviedb?authSource=admin
    volumes:
      - ../../:/app
    env_file:
      - ../../.env
      - .env
    depends_on:
      spark-master:
        condition: service_healthy
    command: >
      bash -c "
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
      "

  # ========================================
  # MongoDB Services
  # ========================================
  
  mongo:
    image: mongo:7.0
    container_name: batch_mongo
    hostname: mongo
    networks:
      - movie_batch_net
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
      - ./docker/mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro
    environment:
      - MONGO_INITDB_ROOT_USERNAME=admin
      - MONGO_INITDB_ROOT_PASSWORD=password
      - MONGO_INITDB_DATABASE=moviedb
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 40s

  mongo-express:
    image: mongo-express:1.0-20
    container_name: batch_mongo_express
    hostname: mongo-express
    networks:
      - movie_batch_net
    ports:
      - "8082:8081"
    environment:
      - ME_CONFIG_MONGODB_ADMINUSERNAME=admin
      - ME_CONFIG_MONGODB_ADMINPASSWORD=password
      - ME_CONFIG_MONGODB_URL=mongodb://admin:password@mongo:27017/
      - ME_CONFIG_BASICAUTH_USERNAME=admin
      - ME_CONFIG_BASICAUTH_PASSWORD=admin
    depends_on:
      mongo:
        condition: service_healthy

  # ========================================
  # Airflow Services
  # ========================================
  
  postgres:
    image: postgres:15
    container_name: batch_postgres
    hostname: postgres
    networks:
      - movie_batch_net
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      timeout: 5s
      retries: 5

  airflow-init:
    build:
      context: ../../
      dockerfile: layers/batch_layer/docker/airflow.Dockerfile
    container_name: batch_airflow_init
    networks:
      - movie_batch_net
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=81HqDtbqAywKSOumSha3BhWNOdQ26slT6K0YaZeZyPs=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - AIRFLOW__WEBSERVER__SECRET_KEY=super_secret_key_12345
      - HDFS_NAMENODE=hdfs://namenode:8020
      - SPARK_MASTER_URL=spark://spark-master:7077
      - MONGODB_CONNECTION_STRING=mongodb://admin:password@mongo:27017/moviedb?authSource=admin
      - TMDB_API_KEY=${TMDB_API_KEY}
      - AIRFLOW_PROJECT_ROOT=/app
      - _AIRFLOW_DB_MIGRATE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
    volumes:
      - ../../:/app
      - airflow_logs:/opt/airflow/logs
      - ./docker/airflow-init.sh:/docker-entrypoint-init.d/airflow-init.sh:ro
    env_file:
      - ../../.env
      - .env
    depends_on:
      postgres:
        condition: service_healthy
      mongo:
        condition: service_healthy
      namenode:
        condition: service_healthy
      spark-master:
        condition: service_healthy
    entrypoint: /bin/bash
    command: >
      -c "
        airflow db migrate &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true &&
        /docker-entrypoint-init.d/airflow-init.sh
      "

  airflow-webserver:
    build:
      context: ../../
      dockerfile: layers/batch_layer/docker/airflow.Dockerfile
    container_name: batch_airflow_webserver
    hostname: airflow-webserver
    networks:
      - movie_batch_net
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=81HqDtbqAywKSOumSha3BhWNOdQ26slT6K0YaZeZyPs=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - AIRFLOW__WEBSERVER__SECRET_KEY=super_secret_key_12345
      - HDFS_NAMENODE=hdfs://namenode:8020
      - SPARK_MASTER_URL=spark://spark-master:7077
      - MONGODB_CONNECTION_STRING=mongodb://admin:password@mongo:27017/moviedb?authSource=admin
      - TMDB_API_KEY=${TMDB_API_KEY}
      - AIRFLOW_PROJECT_ROOT=/app
    volumes:
      - ../../dags:/opt/airflow/dags
      - ../../layers:/opt/airflow/layers
      - ../../config:/opt/airflow/config
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
      - ../../:/app
    env_file:
      - ../../.env
      - .env
    depends_on:
      - airflow-init
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    command: webserver

  airflow-scheduler:
    build:
      context: ../../
      dockerfile: layers/batch_layer/docker/airflow.Dockerfile
    container_name: batch_airflow_scheduler
    hostname: airflow-scheduler
    networks:
      - movie_batch_net
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=81HqDtbqAywKSOumSha3BhWNOdQ26slT6K0YaZeZyPs=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - HDFS_NAMENODE=hdfs://namenode:8020
      - SPARK_MASTER_URL=spark://spark-master:7077
      - MONGODB_CONNECTION_STRING=mongodb://admin:password@mongo:27017/moviedb?authSource=admin
      - TMDB_API_KEY=${TMDB_API_KEY}
      - AIRFLOW_PROJECT_ROOT=/app
    volumes:
      - ../../dags:/opt/airflow/dags
      - ../../layers:/opt/airflow/layers
      - ../../config:/opt/airflow/config
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
      - ../../:/app
    env_file:
      - ../../.env
      - .env
    depends_on:
      - airflow-init
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    command: scheduler
